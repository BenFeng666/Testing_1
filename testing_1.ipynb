{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyNXku7uKnZHouHWvgKFrfcZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "10b918d279f544dcbc761338687cd5f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3cee64380ff14cd09a363c43b35e08bb",
              "IPY_MODEL_551a3c9e1dd24a72af4b8f6074260ccb",
              "IPY_MODEL_0de5a0164fd3442d91f32417d127ba4d"
            ],
            "layout": "IPY_MODEL_fa9298ca60a645bbb8f8e2391a70f418"
          }
        },
        "3cee64380ff14cd09a363c43b35e08bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7b75cbeb28a848a0958f70beac7ff8fe",
            "placeholder": "​",
            "style": "IPY_MODEL_7da22eb89bff4c5da827b81259c8f6a0",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "551a3c9e1dd24a72af4b8f6074260ccb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aecca1bc2ecb49e3a8df624fe20d85c3",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_540e853f587a45de827ad4cdfb14b0c3",
            "value": 2
          }
        },
        "0de5a0164fd3442d91f32417d127ba4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_56a78108ed0640a1b636bea687dc93b5",
            "placeholder": "​",
            "style": "IPY_MODEL_4936a87d3d4a44949d5914ed7d6c0832",
            "value": " 2/2 [00:04&lt;00:00,  2.21s/it]"
          }
        },
        "fa9298ca60a645bbb8f8e2391a70f418": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b75cbeb28a848a0958f70beac7ff8fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7da22eb89bff4c5da827b81259c8f6a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aecca1bc2ecb49e3a8df624fe20d85c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "540e853f587a45de827ad4cdfb14b0c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "56a78108ed0640a1b636bea687dc93b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4936a87d3d4a44949d5914ed7d6c0832": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BenFeng666/Testing_1/blob/main/testing_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install compatible stack for torch 2.8.0\n",
        "!pip install -q bitsandbytes==0.42.0\n",
        "!pip install -q triton==3.4.0\n",
        "!pip install -q accelerate transformers einops\n",
        "!pip install -q torchvision\n"
      ],
      "metadata": {
        "id": "-RXKuhZvZBXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sAyLeilh4X5c",
        "outputId": "e50eb8cf-962e-42a6-cf58-3b3982f2efa2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.56.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.34.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.9)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.8.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y torch torchvision torchaudio bitsandbytes\n",
        "!pip install torch==2.1.2+cu118 torchvision==0.16.2+cu118 torchaudio==2.1.2+cu118 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install bitsandbytes==0.43.1\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "B0Yx0iUW8ZlC",
        "outputId": "fb90d0ca-38ee-483c-9d27-babfb229f228"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 2.8.0\n",
            "Uninstalling torch-2.8.0:\n",
            "  Successfully uninstalled torch-2.8.0\n",
            "Found existing installation: torchvision 0.23.0\n",
            "Uninstalling torchvision-0.23.0:\n",
            "  Successfully uninstalled torchvision-0.23.0\n",
            "\u001b[33mWARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping torchaudio as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: bitsandbytes 0.42.0\n",
            "Uninstalling bitsandbytes-0.42.0:\n",
            "  Successfully uninstalled bitsandbytes-0.42.0\n",
            "\u001b[33mWARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement torch==2.1.2+cu118 (from versions: 2.2.0, 2.2.0+cpu, 2.2.0+cpu.cxx11.abi, 2.2.0+cu118, 2.2.0+cu121, 2.2.0+rocm5.6, 2.2.0+rocm5.7, 2.2.1, 2.2.1+cpu, 2.2.1+cpu.cxx11.abi, 2.2.1+cu118, 2.2.1+cu121, 2.2.1+rocm5.6, 2.2.1+rocm5.7, 2.2.2, 2.2.2+cpu, 2.2.2+cpu.cxx11.abi, 2.2.2+cu118, 2.2.2+cu121, 2.2.2+rocm5.6, 2.2.2+rocm5.7, 2.3.0, 2.3.0+cpu, 2.3.0+cpu.cxx11.abi, 2.3.0+cu118, 2.3.0+cu121, 2.3.0+rocm5.7, 2.3.0+rocm6.0, 2.3.1, 2.3.1+cpu, 2.3.1+cpu.cxx11.abi, 2.3.1+cu118, 2.3.1+cu121, 2.3.1+rocm5.7, 2.3.1+rocm6.0, 2.4.0, 2.4.1, 2.5.0, 2.5.1, 2.6.0, 2.7.0, 2.7.1, 2.8.0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for torch==2.1.2+cu118\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting bitsandbytes==0.43.1\n",
            "  Using cached bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl.metadata (2.2 kB)\n",
            "Collecting torch (from bitsandbytes==0.43.1)\n",
            "  Using cached torch-2.8.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from bitsandbytes==0.43.1) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->bitsandbytes==0.43.1) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->bitsandbytes==0.43.1) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->bitsandbytes==0.43.1) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->bitsandbytes==0.43.1) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->bitsandbytes==0.43.1) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->bitsandbytes==0.43.1) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->bitsandbytes==0.43.1) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch->bitsandbytes==0.43.1) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch->bitsandbytes==0.43.1) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch->bitsandbytes==0.43.1) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->bitsandbytes==0.43.1) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->bitsandbytes==0.43.1) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch->bitsandbytes==0.43.1) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch->bitsandbytes==0.43.1) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch->bitsandbytes==0.43.1) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch->bitsandbytes==0.43.1) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->bitsandbytes==0.43.1) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->bitsandbytes==0.43.1) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch->bitsandbytes==0.43.1) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch->bitsandbytes==0.43.1) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch->bitsandbytes==0.43.1) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->bitsandbytes==0.43.1) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->bitsandbytes==0.43.1) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->bitsandbytes==0.43.1) (3.0.2)\n",
            "Using cached bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\n",
            "Using cached torch-2.8.0-cp312-cp312-manylinux_2_28_x86_64.whl (887.9 MB)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mInstalling collected packages: torch, bitsandbytes\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "timm 1.0.19 requires torchvision, which is not installed.\n",
            "fastai 2.8.4 requires torchvision>=0.11, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed bitsandbytes-0.43.1 torch-2.8.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch",
                  "torchgen"
                ]
              },
              "id": "917a57b01deb4643b98562491a8d4ff6"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
        "import torch\n",
        "\n",
        "model_name_or_id = \"AI4Chem/ChemLLM-7B-Chat\"\n",
        "\n",
        "# Must be True, otherwise the model won't load\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name_or_id,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name_or_id, trust_remote_code=True)\n",
        "\n",
        "prompt = \"What is Molecule of Ibuprofen?\"\n",
        "enc = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "# Custom lightweight generate loop (avoids buggy past_key_values)\n",
        "def simple_generate(model, tokenizer, enc, max_new_tokens=200):\n",
        "    model.eval()\n",
        "    input_ids = enc[\"input_ids\"].clone()\n",
        "    attention_mask = enc[\"attention_mask\"].clone()\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "        with torch.inference_mode():\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits[:, -1, :]\n",
        "            next_token = torch.argmax(logits, dim=-1).unsqueeze(-1)\n",
        "\n",
        "        input_ids = torch.cat([input_ids, next_token], dim=-1)\n",
        "        attention_mask = torch.cat(\n",
        "            [attention_mask, torch.ones_like(next_token)], dim=-1\n",
        "        )\n",
        "\n",
        "        if next_token.item() == tokenizer.eos_token_id:\n",
        "            break\n",
        "\n",
        "    return tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
        "\n",
        "# Run it\n",
        "output_text = simple_generate(model, tokenizer, enc, max_new_tokens=200)\n",
        "print(output_text)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208,
          "referenced_widgets": [
            "10b918d279f544dcbc761338687cd5f4",
            "3cee64380ff14cd09a363c43b35e08bb",
            "551a3c9e1dd24a72af4b8f6074260ccb",
            "0de5a0164fd3442d91f32417d127ba4d",
            "fa9298ca60a645bbb8f8e2391a70f418",
            "7b75cbeb28a848a0958f70beac7ff8fe",
            "7da22eb89bff4c5da827b81259c8f6a0",
            "aecca1bc2ecb49e3a8df624fe20d85c3",
            "540e853f587a45de827ad4cdfb14b0c3",
            "56a78108ed0640a1b636bea687dc93b5",
            "4936a87d3d4a44949d5914ed7d6c0832"
          ]
        },
        "id": "cedLX4wF4pZ1",
        "outputId": "2adbd207-e30f-4680-c309-649cd712a624"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "10b918d279f544dcbc761338687cd5f4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is Molecule of Ibuprofen? – Molecule of Ibuprofen is the chemical compound of C13H18O2. It is a non-steroidal anti-inflammatory drug (NSAID) used to treat pain, fever, and inflammation. It is also used to treat menstrual cramps, migraines, and rheumatoid arthritis. Ibuprofen is available as over-the-counter (OTC) medication, as well as by prescription. It is available in various forms, including tablets, capsules, liquid, and suppositories. Ibuprofen is a member of the class of drugs known as non-steroidal anti-inflammatory drugs (NSAIDs). NSAIDs are used to treat pain, inflammation, and fever. They work by blocking the production of prostaglandins, which are chemicals that cause pain, inflammation, and fever. Ibuprofen is one of the most widely used NSAIDs. It is available as over-the-counter (OT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab import files\n",
        "import json\n",
        "\n",
        "# Upload the file\n",
        "uploaded = files.upload()\n",
        "filename = list(uploaded.keys())[0]\n",
        "\n",
        "# Read the Excel file, using the 2nd row as header\n",
        "df = pd.read_excel(filename, header=1)\n",
        "\n",
        "# Clean up column names (remove stray spaces)\n",
        "df.columns = df.columns.str.strip()\n",
        "\n",
        "# Keep only needed columns\n",
        "df = df[[\"Structure\", \"transfection efficiency (MC3)\", \"Score\"]]\n",
        "df = df.rename(columns={\n",
        "    \"Structure\": \"smiles\",\n",
        "    \"transfection efficiency (MC3)\": \"rate\",\n",
        "    \"Score\": \"score\"\n",
        "})\n",
        "\n",
        "# Drop rows with missing values\n",
        "df = df.dropna().reset_index(drop=True)\n",
        "\n",
        "# === Only take the first 210 labeled rows ===\n",
        "df = df.iloc[:210].copy()\n",
        "\n",
        "# Train-test split: 150 train, 50 test\n",
        "train_df, test_df = train_test_split(df, test_size=50, random_state=42, shuffle=True)\n",
        "\n",
        "# Reset indices\n",
        "train_df = train_df.reset_index(drop=True)\n",
        "test_df  = test_df.reset_index(drop=True)\n",
        "\n",
        "print(\"Train size:\", len(train_df), \" Test size:\", len(test_df))\n",
        "\n",
        "# Convert to JSON (keep score as float)\n",
        "train_json = [\n",
        "    {\n",
        "        \"id\": str(i+1),\n",
        "        \"smiles\": r.smiles,\n",
        "        \"transfection_efficiency\": float(r.rate),\n",
        "        \"score\": float(r.score)\n",
        "    }\n",
        "    for i, r in train_df.iterrows()\n",
        "]\n",
        "\n",
        "test_json = [\n",
        "    {\n",
        "        \"id\": str(i+1),\n",
        "        \"smiles\": r.smiles,\n",
        "        \"transfection_efficiency\": float(r.rate),\n",
        "        \"score\": float(r.score)\n",
        "    }\n",
        "    for i, r in test_df.iterrows()\n",
        "]\n",
        "\n",
        "# Save to files\n",
        "with open(\"train.json\", \"w\") as f:\n",
        "    json.dump(train_json, f, indent=2)\n",
        "\n",
        "with open(\"test.json\", \"w\") as f:\n",
        "    json.dump(test_json, f, indent=2)\n",
        "\n",
        "print(\"Sample train:\", train_json[:2])\n",
        "print(\"Sample test:\", test_json[:2])\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "id": "jwqPItYv5s5Q",
        "outputId": "e18514a2-98b1-4700-e85d-47e5706ce83c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-3b6cb84b-aa00-4063-b13f-609185b6c1cd\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-3b6cb84b-aa00-4063-b13f-609185b6c1cd\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Smiles file-Published data.xlsx to Smiles file-Published data (2).xlsx\n",
            "Train size: 160  Test size: 50\n",
            "Sample train: [{'id': '1', 'smiles': '[H][C@@]1(OC[C@@H]2CCCCN(CCCCCCCCCCCCCC)CCCCCCCCCCCCCC)[C@@]2([H])OC[C@H]1OCCCN(CCCCCCCCCCCCCC)CCCCCCCCCCCCCC', 'transfection_efficiency': 0.355288614585025, 'score': 1.0}, {'id': '2', 'smiles': 'O=C1C(C(OCCCC)=O)CN(CCCN2CCCC2)CC1C(OCCCC)=O', 'transfection_efficiency': 3.034227, 'score': 1.0}]\n",
            "Sample test: [{'id': '1', 'smiles': 'OC1=CC=CC=C1CN(CCCN(C)C)CC2=C(O)C=CC=C2', 'transfection_efficiency': 2.900367, 'score': 1.0}, {'id': '2', 'smiles': 'O=C1C(C(OCCCC)=O)CN(CC)CC1C(OCCCC)=O', 'transfection_efficiency': 1.20412, 'score': 1.0}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SCORE_SCALE = \"\"\"\n",
        "Score reference table:\n",
        "0 – Slight mRNA transfection efficiency from crude lipid library (0–0.1 fold vs commercial lipid, in vitro data).\n",
        "1 – Slight mRNA transfection efficiency from purified lipid library (0–0.1 fold vs commercial lipid, in vitro data).\n",
        "2 – Obvious mRNA transfection efficiency from crude lipid library, less than commercial lipid (0.1–1 fold, in vitro).\n",
        "3 – Obvious mRNA transfection efficiency from purified lipid library, less than commercial lipid (0.1–1 fold, in vitro).\n",
        "4 – Obvious mRNA transfection efficiency from crude lipid library, better than commercial lipid (>1 fold), not final candidates (no purified lipid data).\n",
        "5 – Obvious mRNA transfection efficiency from purified lipid library, better than commercial lipid (>1 fold, in vitro).\n",
        "6 – Top lipid for in vivo study, not better than commercial lipids (MC3, ALC-0315, SM102).\n",
        "7 – Top lipid for in vivo study, better than MC3 and ALC-0315.\n",
        "8 – Top lipid for in vivo study, better than SM102.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "6yty800zb-uZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, json, re\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "# ==== 1. Define system prompt with examples + score scale ====\n",
        "K = 100\n",
        "few_examples = train_json[:K]\n",
        "\n",
        "SCORE_SCALE = \"\"\"\n",
        "Score reference table:\n",
        "0 – Slight mRNA transfection efficiency from crude lipid library (0–0.1 fold vs commercial lipid, in vitro data).\n",
        "1 – Slight mRNA transfection efficiency from purified lipid library (0–0.1 fold vs commercial lipid, in vitro data).\n",
        "2 – Obvious efficiency from crude lipid library, less than commercial lipid (0.1–1 fold, in vitro).\n",
        "3 – Obvious efficiency from purified lipid library, less than commercial lipid (0.1–1 fold, in vitro).\n",
        "4 – Obvious efficiency from crude lipid library, better than commercial lipid (>1 fold), not final candidates (no purified lipid data).\n",
        "5 – Obvious efficiency from purified lipid library, better than commercial lipid (>1 fold, in vitro).\n",
        "6 – Top lipid for in vivo study, not better than commercial lipids (MC3, ALC-0315, SM102).\n",
        "7 – Top lipid for in vivo study, better than MC3 and ALC-0315.\n",
        "8 – Top lipid for in vivo study, better than SM102.\n",
        "\"\"\"\n",
        "\n",
        "SYSTEM_PROMPT = (\n",
        "    \"You are a medicinal chemistry and drug delivery assistant.\\n\\n\"\n",
        "    \"Context:\\n\"\n",
        "    \"I will provide a JSON array of training examples from an assay.\\n\"\n",
        "    \"Each item is {\\\"id\\\": \\\"...\\\", \\\"smiles\\\": \\\"...\\\", \"\n",
        "    \"\\\"transfection_efficiency\\\": <float>, \\\"score\\\": <int>}.\\n\\n\"\n",
        "    f\"Training examples:\\n{json.dumps(few_examples, indent=2)}\\n\\n\"\n",
        "    f\"Score scale:\\n{SCORE_SCALE}\\n\\n\"\n",
        "    \"Task:\\n\"\n",
        "    \"Given the following target SMILES, predict:\\n\"\n",
        "    \"1. A numeric transfection_efficiency (float).\\n\"\n",
        "    \"2. A score (integer 0–8) based on the scale.\\n\"\n",
        "    \"3. A confidence value (1–10) for how certain you are about the predicted score.\\n\"\n",
        "    \"4. A brief rationale (2–5 sentences).\\n\\n\"\n",
        "    \"IMPORTANT OUTPUT RULES:\\n\"\n",
        "    \"– Line 1: ONLY the float number.\\n\"\n",
        "    \"– Line 2: ONLY the score (integer).\\n\"\n",
        "    \"– Line 3: ONLY the confidence (1–10).\\n\"\n",
        "    \"– Lines 4–8: rationale sentences.\\n\"\n",
        "    \"– Do NOT write JSON or code.\\n\"\n",
        ")\n",
        "\n",
        "# ==== 2. Helper: build model inputs ====\n",
        "def build_inputs(smiles: str):\n",
        "    try:\n",
        "        prompt_text = tokenizer.apply_chat_template(\n",
        "            [\n",
        "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "                {\"role\": \"user\", \"content\": f\"SMILES: {smiles}\"}\n",
        "            ],\n",
        "            add_generation_prompt=True,\n",
        "            tokenize=False\n",
        "        )\n",
        "    except (ValueError, AttributeError):\n",
        "        prompt_text = SYSTEM_PROMPT + f\"\\n\\nSMILES: {smiles}\\n\"\n",
        "\n",
        "    enc = tokenizer(prompt_text, return_tensors=\"pt\")\n",
        "    enc = {k: v.to(\"cuda\") for k, v in enc.items()}\n",
        "\n",
        "    if tokenizer.pad_token_id is None:\n",
        "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "    if getattr(model.generation_config, \"pad_token_id\", None) is None:\n",
        "        model.generation_config.pad_token_id = tokenizer.eos_token_id\n",
        "    return enc\n",
        "\n",
        "# ==== 3. Run test predictions ====\n",
        "for i, sample in enumerate(test_json[:20], start=1):\n",
        "    query_smiles = sample[\"smiles\"]\n",
        "    true_eff = sample[\"transfection_efficiency\"]\n",
        "    true_score = sample[\"score\"]\n",
        "\n",
        "    enc = build_inputs(query_smiles)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "        out = model.generate(\n",
        "            **enc,\n",
        "            max_new_tokens=200,\n",
        "            do_sample=False,\n",
        "            use_cache=False\n",
        "        )\n",
        "\n",
        "    input_len = enc[\"input_ids\"].shape[1]\n",
        "    gen_text = tokenizer.decode(out[0, input_len:], skip_special_tokens=True).strip()\n",
        "    lines = [l.strip() for l in gen_text.splitlines() if l.strip()]\n",
        "\n",
        "    pred_eff, pred_score, pred_conf, rationale = None, None, None, \"\"\n",
        "\n",
        "    if len(lines) >= 1:\n",
        "        m = re.search(r\"[-+]?\\d*\\.?\\d+(?:[eE][-+]?\\d+)?\", lines[0])\n",
        "        if m:\n",
        "            try: pred_eff = float(m.group(0))\n",
        "            except: pass\n",
        "    if len(lines) >= 2:\n",
        "        m = re.search(r\"\\d+\", lines[1])\n",
        "        if m:\n",
        "            try: pred_score = int(m.group(0))\n",
        "            except: pass\n",
        "    if len(lines) >= 3:\n",
        "        m = re.search(r\"\\d+\", lines[2])\n",
        "        if m:\n",
        "            try: pred_conf = int(m.group(0))\n",
        "            except: pass\n",
        "    if len(lines) > 3:\n",
        "        rationale = \"\\n\".join(lines[3:]).strip()\n",
        "\n",
        "    # === Print result ===\n",
        "    print(f\"\\n=== Sample {i} ===\")\n",
        "    print(\"SMILES:\", query_smiles)\n",
        "    print(f\"Predicted efficiency: {pred_eff}\")\n",
        "    print(f\"Actual efficiency:    {true_eff}\")\n",
        "    print(f\"Predicted score:      {pred_score}\")\n",
        "    print(f\"Actual score:         {true_score}\")\n",
        "    print(f\"Confidence (1–10):    {pred_conf}\")\n",
        "    print(\"Rationale:\", rationale)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D00N8sk6cEqP",
        "outputId": "c83557f4-5f47-4f18-ba56-606437aa325d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Sample 1 ===\n",
            "SMILES: OC1=CC=CC=C1CN(CCCN(C)C)CC2=C(O)C=CC=C2\n",
            "Predicted efficiency: 0.0\n",
            "Actual efficiency:    2.900367\n",
            "Predicted score:      0\n",
            "Actual score:         1.0\n",
            "Confidence (1–10):    1\n",
            "Rationale: Rationale: The SMILES provided is a lipid-like molecule. However, the SMILES does not contain any information about the lipid structure, so it is not possible to predict the transfection efficiency. Therefore, the score is 0, the confidence is 1, and the rationale is \"The SMILES provided is a lipid-like molecule. However, the SMILES does not contain any information about the lipid structure, so it is not possible to predict the transfection efficiency. Therefore, the score is 0, the confidence is 1, and the rationale is 'The SMILES provided is a lipid-like molecule. However, the SMILES does not contain any information about the lipid structure, so it is not possible to predict the transfection efficiency. Therefore, the score is 0, the confidence is 1, and the rationale is '\".\n",
            "\n",
            "=== Sample 2 ===\n",
            "SMILES: O=C1C(C(OCCCC)=O)CN(CC)CC1C(OCCCC)=O\n",
            "Predicted efficiency: 3.172019\n",
            "Actual efficiency:    1.20412\n",
            "Predicted score:      2\n",
            "Actual score:         1.0\n",
            "Confidence (1–10):    6\n",
            "Rationale: Rationale: This lipid shows a slight mRNA transfection efficiency from crude lipid library, but is less than the commercial lipid. However, the efficiency is still better than the average of the crude lipid library. Therefore, the score is 2. The confidence is 6 because the efficiency is slightly better than the average of the crude lipid library.\n",
            "\n",
            "=== Sample 3 ===\n",
            "SMILES: OC1=CC=C(CCCCCCCCC)C=C1CN(CCCN(CC2=CC(CCCCCCCCC)=CC=C2O)CC3=CC(CCCCCCCCC)=CC=C3O)CC4=C(O)C=CC(CCCCCCCCC)=C4\n",
            "Predicted efficiency: 6.215218\n",
            "Actual efficiency:    4.722987\n",
            "Predicted score:      4\n",
            "Actual score:         2.0\n",
            "Confidence (1–10):    9\n",
            "Rationale: Rationale: The SMILES provided is a lipid sequence that is expected to have a transfection efficiency of 6.215218. This is considered to be a good score, as it is above the average of 1.0. The confidence of this prediction is 9, indicating a high degree of certainty. This lipid sequence is expected to be a good candidate for further study.\n",
            "\n",
            "=== Sample 4 ===\n",
            "SMILES: O=C1C(C(OCCCCCCCC)=O)CN(CN(C)C)CC1C(OCCCCCCCC)=O\n",
            "Predicted efficiency: 0.0\n",
            "Actual efficiency:    2.795185\n",
            "Predicted score:      0\n",
            "Actual score:         1.0\n",
            "Confidence (1–10):    1\n",
            "Rationale: Rationale: The transfection efficiency of this lipid is very low, as the transfection efficiency is 0.0. This is likely due to the lack of cationic lipid groups, which are important for binding to the negatively charged mRNA. Therefore, this lipid is not suitable for mRNA transfection.\n",
            "\n",
            "=== Sample 5 ===\n",
            "SMILES: OC1=CC=C(CCCCCC)C=C1CN(CCCCCCN2CCN(CCNCCCCCCN(CC3=CC(CCCCCC)=CC=C3O)CC4=CC(CCCCCC)=CC=C4O)CC2)CC5=C(O)C=CC(CCCCCC)=C5\n",
            "Predicted efficiency: 6.215218\n",
            "Actual efficiency:    5.271137\n",
            "Predicted score:      4\n",
            "Actual score:         3.0\n",
            "Confidence (1–10):    9\n",
            "Rationale: Rationale: The SMILES provided is a complex lipidic compound. The compound is expected to have a good transfection efficiency. The predicted score is 4, which indicates that the compound is expected to be better than commercial lipids but not as good as the top lipids. The confidence value is 9, indicating a high confidence in the prediction. The rationale is that the compound is expected to have a good transfection efficiency due to the presence of multiple lipidic chains and the presence of multiple cationic groups.\n",
            "\n",
            "=== Sample 6 ===\n",
            "SMILES: CCCCCCCCCCCCSCN(CCCN(CCCN(CSCCCCCCCCCCCC)CSCCCCCCCCCCCC)CCCN(CSCCCCCCCCCCCC)CSCCCCCCCCCCCC)CSCCCCCCCCCCCC\n",
            "Predicted efficiency: None\n",
            "Actual efficiency:    3.368845\n",
            "Predicted score:      1\n",
            "Actual score:         1.0\n",
            "Confidence (1–10):    2\n",
            "Rationale: 3. 1\n",
            "4.\n",
            "5.\n",
            "6.\n",
            "7.\n",
            "8.\n",
            "Rationale: This is a crude lipid library, so the transfection efficiency is expected to be very low. However, the lipid is still able to form lipoplexes, so it is not completely ineffective. The score is 0, indicating that this is not a good candidate for further development. The confidence is 1, indicating that the prediction is very uncertain.\n",
            "\n",
            "=== Sample 7 ===\n",
            "SMILES: OC1=CC=CC=C1CN(CCCCC2NC(C(CCCCN(CC3=CC=CC=C3O)CC4=CC=CC=C4O)NC2=O)=O)CC5=C(O)C=CC=C5\n",
            "Predicted efficiency: 0.0\n",
            "Actual efficiency:    3.200577\n",
            "Predicted score:      0\n",
            "Actual score:         1.0\n",
            "Confidence (1–10):    1\n",
            "Rationale: Rationale: The SMILES provided is a complex lipid-like molecule. The transfection efficiency of this molecule is expected to be very low, as the SMILES does not contain any known lipid-like structures. Therefore, the score is 0, and the confidence is 1.\n",
            "\n",
            "=== Sample 8 ===\n",
            "SMILES: O=C1C(C(OCCCC)=O)CN(CCN2CCCC2)CC1C(OCCCC)=O\n",
            "Predicted efficiency: 2.951823\n",
            "Actual efficiency:    2.755875\n",
            "Predicted score:      2\n",
            "Actual score:         1.0\n",
            "Confidence (1–10):    6\n",
            "Rationale: Rationale: This lipid shows a moderate transfection efficiency, but is still within the range of the crude lipid library. It is not as efficient as the commercial lipids, but is still a good candidate for further optimization.\n",
            "\n",
            "=== Sample 9 ===\n",
            "SMILES: [H][C@]1(OC[C@H]2CCCCN(CCCCCCOC(C)OCCCCCC)CCCCCCOC(C)OCCCCCC)[C@]2([H])OC[C@@H]1OCCCN(CCCCCCOC(C)OCCCCCC)CCCCCCOC(C)OCCCCCC\n",
            "Predicted efficiency: 0.422099561474744\n",
            "Actual efficiency:    1.74017974121596\n",
            "Predicted score:      4\n",
            "Actual score:         2.0\n",
            "Confidence (1–10):    6\n",
            "Rationale: Rationale: This lipid shows a moderate mRNA transfection efficiency from crude lipid library, but is not as efficient as the commercial lipids. However, it is still a good candidate for further optimization.\n",
            "\n",
            "=== Sample 10 ===\n",
            "SMILES: O=C1C(C(OCCCCCC)=O)CN(CCCN2CCCC2)CC1C(OCCCCCC)=O\n",
            "Predicted efficiency: 0.0\n",
            "Actual efficiency:    3.134814\n",
            "Predicted score:      0\n",
            "Actual score:         1.0\n",
            "Confidence (1–10):    1\n",
            "Rationale: Rationale: The transfection efficiency of this lipid is very low, as it is only 0.0. This is likely due to the fact that the lipid does not form stable liposomes, which are necessary for efficient transfection. Therefore, this lipid is not suitable for use as a transfection reagent.\n",
            "\n",
            "=== Sample 11 ===\n",
            "SMILES: CCCCCCCCSCN(CCCN1CCN(CCCN(CSCCCCCCCC)CSCCCCCCCC)CC1)CSCCCCCCCC\n",
            "Predicted efficiency: 0.64\n",
            "Actual efficiency:    4.726303\n",
            "Predicted score:      4\n",
            "Actual score:         2.0\n",
            "Confidence (1–10):    6\n",
            "Rationale: Rationale: The lipid is a cationic lipid, which is expected to be able to transfect mRNA. However, the transfection efficiency is only 0.64, which is lower than the commercial lipids. The score is 4, indicating that the lipid is better than the crude lipid library but not as good as the purified lipid library. The confidence is 6, indicating that the prediction is moderately confident.\n",
            "\n",
            "=== Sample 12 ===\n",
            "SMILES: O=C1C(C(OCCCCCC)=O)CN(CN2CCN(C)CC2)CC1C(OCCCCCC)=O\n",
            "Predicted efficiency: 0.9\n",
            "Actual efficiency:    2.777427\n",
            "Predicted score:      2\n",
            "Actual score:         1.0\n",
            "Confidence (1–10):    6\n",
            "Rationale: Rationale: The transfection efficiency of this lipid is slightly better than the crude lipid library, but still less than the commercial lipid. However, the purified lipid library shows a significant improvement in transfection efficiency. This lipid is not yet a final candidate, but it is worth further investigation.\n",
            "\n",
            "=== Sample 13 ===\n",
            "SMILES: O=C1C(C(OCCCCCCCC)=O)CN(CCCN(CC)CC)CC1C(OCCCCCCCC)=O\n",
            "Predicted efficiency: 2.9\n",
            "Actual efficiency:    3.283753\n",
            "Predicted score:      2\n",
            "Actual score:         1.0\n",
            "Confidence (1–10):    6\n",
            "Rationale: Rationale: This lipid shows a moderate transfection efficiency, but is still better than the commercial lipid. The confidence is 6 because the transfection efficiency is close to the cutoff for the score of 2.\n",
            "\n",
            "=== Sample 14 ===\n",
            "SMILES: CN(C)CCCN(CSCCCCCCCC)CSCCCCCCCC\n",
            "Predicted efficiency: 2.371068\n",
            "Actual efficiency:    2.164353\n",
            "Predicted score:      1\n",
            "Actual score:         1.0\n",
            "Confidence (1–10):    6\n",
            "Rationale: Rationale: The SMILES is a lipid-like molecule. This molecule is a member of the class of 1,3-diazinanes that is 1,3-diazinane substituted by 2-(dimethylamino)ethyl groups at positions 1 and 3. This compound is a lipid-like molecule. This molecule is a member of the class of 1,3-diazinanes that is 1,3-diazinane substituted by 2-(dimethylamino)ethyl groups at positions 1 and 3. This compound is a lipid-like molecule. This molecule is a member of the class of 1,3-diazinanes that is 1,3-diazinane substituted by 2-(dimethylamino)ethyl groups at positions 1 and 3. This compound is\n",
            "\n",
            "=== Sample 15 ===\n",
            "SMILES: [H][C@]1(OC[C@H]2CCCCN(CCCCCCCCCCCCCCCC)CCCCCCCCCCCCCCCC)[C@]2([H])OC[C@H]1OCCCN(CCCCCCCCCCCCCCCC)CCCCCCCCCCCCCCCC\n",
            "Predicted efficiency: 0.000719181419522495\n",
            "Actual efficiency:    0.0938531752476856\n",
            "Predicted score:      1\n",
            "Actual score:         1.0\n",
            "Confidence (1–10):    1\n",
            "Rationale: Rationale: The transfection efficiency of this lipid is very low, as the transfection score is 1. This is likely due to the large number of cationic groups on the lipid, which can lead to aggregation and poor solubility. However, the lipid does have a high number of cationic groups, which could potentially improve transfection efficiency if the lipid is purified to remove aggregates.\n",
            "\n",
            "=== Sample 16 ===\n",
            "SMILES: OC1=CC=C(CCCCCCCC)C=C1CN(CCN2CCN(CNCCN(CC3=CC(CCCCCCCC)=CC=C3O)CC4=CC(CCCCCCCC)=CC=C4O)CC2)CC5=C(O)C=CC(CCCCCCCC)=C5\n",
            "Predicted efficiency: 6.215218\n",
            "Actual efficiency:    6.036851\n",
            "Predicted score:      4\n",
            "Actual score:         4.0\n",
            "Confidence (1–10):    9\n",
            "Rationale: Rationale: The SMILES provided is a complex lipidic compound. The compound is a cationic lipid, which is a lipid that is positively charged at physiological pH. Cationic lipids are used as transfection reagents to deliver nucleic acids into cells. Cationic lipids are typically used to deliver nucleic acids into cells, but are not typically used to deliver proteins. The compound is expected to have a transfection efficiency of 6.215218.\n",
            "\n",
            "=== Sample 17 ===\n",
            "SMILES: O=C1C(C(OCC)=O)CN(CCCN(C)C)CC1C(OCC)=O\n",
            "Predicted efficiency: 2.951823\n",
            "Actual efficiency:    2.25042\n",
            "Predicted score:      1\n",
            "Actual score:         1.0\n",
            "Confidence (1–10):    6\n",
            "Rationale: Rationale: The transfection efficiency of this lipid is 2.951823, which is slightly better than the average efficiency of the crude lipid library. However, it is still less than the efficiency of the commercial lipid. Therefore, the score is 1. The confidence is 6 because the difference is not very significant.\n",
            "\n",
            "=== Sample 18 ===\n",
            "SMILES: [H][C@]1(OC[C@H]2CCCCN(CCCCCCOCOCCCCCC)CCCCCCOCOCCCCCC)[C@]2([H])OC[C@H]1OCCCN(CCCCCCOCOCCCCCC)CCCCCCOCOCCCCCC\n",
            "Predicted efficiency: 0.422099561474744\n",
            "Actual efficiency:    0.516163228845217\n",
            "Predicted score:      4\n",
            "Actual score:         1.0\n",
            "Confidence (1–10):    6\n",
            "Rationale: Rationale: This lipid shows a moderate transfection efficiency, but is still better than the commercial lipids. The rationale is that the lipid is able to form a stable lipoplex, but the transfection efficiency is not as high as the top lipids. However, the lipid is still a good candidate for further optimization.\n",
            "\n",
            "=== Sample 19 ===\n",
            "SMILES: O=C1C(C(OCC)=O)CN(CCCN(CC)CC)CC1C(OCC)=O\n",
            "Predicted efficiency: 2.951823\n",
            "Actual efficiency:    3.772542\n",
            "Predicted score:      1\n",
            "Actual score:         1.0\n",
            "Confidence (1–10):    6\n",
            "Rationale: Rationale: The transfection efficiency of this lipid is 2.951823, which is slightly better than the average efficiency of the crude lipid library. However, it is still considered to be a low efficiency lipid. The score is 1, indicating that this lipid is not a good candidate for further development. The confidence value is 6, indicating that there is some uncertainty about the predicted score.\n",
            "\n",
            "=== Sample 20 ===\n",
            "SMILES: OC1=CC=C(CCCCCCCCC)C=C1CN(CCCCC2NC(C(CCCCN(CC3=CC(CCCCCCCCC)=CC=C3O)CC4=CC=C(CCCCCCCCC)C=C4O)NC2=O)=O)CC5=C(O)C=CC(CCCCCCCCC)=C5\n",
            "Predicted efficiency: 6.716817\n",
            "Actual efficiency:    6.173307\n",
            "Predicted score:      4\n",
            "Actual score:         4.0\n",
            "Confidence (1–10):    9\n",
            "Rationale: Rationale: The SMILES provided is a complex lipidic compound. The compound is a lipidic compound having a 1,2-dimyristoyl-sn-glycero-3-phosphoethanolamine (DMPE) moiety, a 1,2-dioleoyl-sn-glycero-3-phosphoethanolamine (DOPE) moiety, and a 1,2-dimyristoyl-sn-glycero-3-phospho-N-methylethanolamine (DMPE) moiety. The compound is expected to be a good candidate for transfection efficiency.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json, torch\n",
        "\n",
        "# Use 150 training examples\n",
        "training_text = json.dumps(train_json, indent=2)\n",
        "\n",
        "SYSTEM_PROMPT_TRAIN = \"\"\"\n",
        "You are a medicinal chemistry and drug delivery assistant.\n",
        "\n",
        "Context:\n",
        "You will first receive 150 training data examples.\n",
        "Each training example has SMILES, transfection_efficiency, and score.\n",
        "\n",
        "When you finish reading the 150 training data, ONLY output: \"yes\".\n",
        "\"\"\"\n",
        "\n",
        "prompt_text = SYSTEM_PROMPT_TRAIN + f\"\\n\\nHere are the 150 training examples:\\n\\n{training_text}\\n\\n\"\n",
        "\n",
        "enc = tokenizer(prompt_text, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "# === Custom generation loop to stop at \"yes\" ===\n",
        "def simple_generate_until_yes(model, tokenizer, enc, max_new_tokens=20):\n",
        "    input_ids = enc[\"input_ids\"]\n",
        "    attention_mask = enc[\"attention_mask\"]\n",
        "\n",
        "    generated = []\n",
        "    for _ in range(max_new_tokens):\n",
        "        with torch.inference_mode():\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits[:, -1, :]\n",
        "            next_token = torch.argmax(logits, dim=-1, keepdim=True)\n",
        "\n",
        "        input_ids = torch.cat([input_ids, next_token], dim=-1)\n",
        "        attention_mask = torch.cat([attention_mask, torch.ones_like(next_token)], dim=-1)\n",
        "\n",
        "        decoded = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
        "\n",
        "        # 🚨 Stop immediately if \"yes\" appears\n",
        "        if \"yes\" in decoded.lower():\n",
        "            return \"yes\"\n",
        "\n",
        "    return decoded.strip()\n",
        "\n",
        "# Run it\n",
        "output_text = simple_generate_until_yes(model, tokenizer, enc, max_new_tokens=20)\n",
        "print(output_text)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2MJFQ97bhPU",
        "outputId": "ec0da79f-0d27-467d-db72-79ab07fd1305"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "yes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re, torch\n",
        "\n",
        "def simple_generate(model, tokenizer, enc, max_new_tokens=120):\n",
        "    input_ids = enc[\"input_ids\"]\n",
        "    attn = enc[\"attention_mask\"]\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "        with torch.inference_mode():\n",
        "            out = model(input_ids=input_ids, attention_mask=attn)\n",
        "            logits = out.logits[:, -1, :]\n",
        "            next_token = torch.argmax(logits, dim=-1, keepdim=True)\n",
        "\n",
        "        input_ids = torch.cat([input_ids, next_token], dim=-1)\n",
        "        attn = torch.cat([attn, torch.ones_like(next_token)], dim=-1)\n",
        "\n",
        "        if next_token.item() == tokenizer.eos_token_id:\n",
        "            break\n",
        "\n",
        "    return tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
        "\n",
        "# === Run test predictions ===\n",
        "for i, sample in enumerate(test_json[:10], start=1):  # adjust [:10] as needed\n",
        "    smiles = sample[\"smiles\"]\n",
        "    true_eff = sample[\"transfection_efficiency\"]\n",
        "    true_score = sample[\"score\"]\n",
        "\n",
        "    enc = build_test_inputs(smiles)\n",
        "    full_text = simple_generate(model, tokenizer, enc, max_new_tokens=120).strip()\n",
        "\n",
        "    # --- keep only *new text after last SMILES: line* ---\n",
        "    if f\"SMILES: {smiles}\" in full_text:\n",
        "        gen_text = full_text.split(f\"SMILES: {smiles}\")[-1].strip()\n",
        "    else:\n",
        "        gen_text = full_text\n",
        "\n",
        "    lines = [l.strip() for l in gen_text.splitlines() if l.strip()]\n",
        "    pred_eff, pred_score, rationale = None, None, \"\"\n",
        "\n",
        "    if len(lines) >= 1:\n",
        "        m = re.search(r\"[-+]?\\d*\\.?\\d+(?:[eE][-+]?\\d+)?\", lines[0])\n",
        "        if m:\n",
        "            try: pred_eff = float(m.group(0))\n",
        "            except: pass\n",
        "    if len(lines) >= 2:\n",
        "        m = re.search(r\"\\d+\", lines[1])\n",
        "        if m:\n",
        "            try: pred_score = int(m.group(0))\n",
        "            except: pass\n",
        "    if len(lines) > 2:\n",
        "        rationale = \"\\n\".join(lines[2:])\n",
        "\n",
        "    # === Print result ===\n",
        "    print(f\"\\n=== Test Sample {i} ===\")\n",
        "    print(\"SMILES:\", smiles)\n",
        "    print(f\"Predicted efficiency: {pred_eff}\")\n",
        "    print(f\"Actual efficiency:    {true_eff}\")\n",
        "    print(f\"Predicted score:      {pred_score}\")\n",
        "    print(f\"Actual score:         {true_score}\")\n",
        "    print(\"Rationale:\", rationale)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4lVnnSqf63b",
        "outputId": "bddcfb4f-ef38-4696-c5c3-2b450c2048ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Test Sample 1 ===\n",
            "SMILES: OC1=CC=CC=C1CN(CCCN(C)C)CC2=C(O)C=CC=C2\n",
            "Predicted efficiency: 0.0\n",
            "Actual efficiency:    2.900367\n",
            "Predicted score:      None\n",
            "Actual score:         1.0\n",
            "Rationale: \n",
            "\n",
            "=== Test Sample 2 ===\n",
            "SMILES: O=C1C(C(OCCCC)=O)CN(CC)CC1C(OCCCC)=O\n",
            "Predicted efficiency: 0.0\n",
            "Actual efficiency:    1.20412\n",
            "Predicted score:      None\n",
            "Actual score:         1.0\n",
            "Rationale: \n",
            "\n",
            "=== Test Sample 3 ===\n",
            "SMILES: OC1=CC=C(CCCCCCCCC)C=C1CN(CCCN(CC2=CC(CCCCCCCCC)=CC=C2O)CC3=CC(CCCCCCCCC)=CC=C3O)CC4=C(O)C=CC(CCCCCCCCC)=C4\n",
            "Predicted efficiency: 0.0\n",
            "Actual efficiency:    4.722987\n",
            "Predicted score:      None\n",
            "Actual score:         2.0\n",
            "Rationale: \n",
            "\n",
            "=== Test Sample 4 ===\n",
            "SMILES: O=C1C(C(OCCCCCCCC)=O)CN(CN(C)C)CC1C(OCCCCCCCC)=O\n",
            "Predicted efficiency: 0.0\n",
            "Actual efficiency:    2.795185\n",
            "Predicted score:      None\n",
            "Actual score:         1.0\n",
            "Rationale: \n",
            "\n",
            "=== Test Sample 5 ===\n",
            "SMILES: OC1=CC=C(CCCCCC)C=C1CN(CCCCCCN2CCN(CCNCCCCCCN(CC3=CC(CCCCCC)=CC=C3O)CC4=CC(CCCCCC)=CC=C4O)CC2)CC5=C(O)C=CC(CCCCCC)=C5\n",
            "Predicted efficiency: 0.0\n",
            "Actual efficiency:    5.271137\n",
            "Predicted score:      None\n",
            "Actual score:         3.0\n",
            "Rationale: \n",
            "\n",
            "=== Test Sample 6 ===\n",
            "SMILES: CCCCCCCCCCCCSCN(CCCN(CCCN(CSCCCCCCCCCCCC)CSCCCCCCCCCCCC)CCCN(CSCCCCCCCCCCCC)CSCCCCCCCCCCCC)CSCCCCCCCCCCCC\n",
            "Predicted efficiency: None\n",
            "Actual efficiency:    3.368845\n",
            "Predicted score:      0\n",
            "Actual score:         1.0\n",
            "Rationale: \n",
            "\n",
            "=== Test Sample 7 ===\n",
            "SMILES: OC1=CC=CC=C1CN(CCCCC2NC(C(CCCCN(CC3=CC=CC=C3O)CC4=CC=CC=C4O)NC2=O)=O)CC5=C(O)C=CC=C5\n",
            "Predicted efficiency: 0.0\n",
            "Actual efficiency:    3.200577\n",
            "Predicted score:      None\n",
            "Actual score:         1.0\n",
            "Rationale: \n",
            "\n",
            "=== Test Sample 8 ===\n",
            "SMILES: O=C1C(C(OCCCC)=O)CN(CCN2CCCC2)CC1C(OCCCC)=O\n",
            "Predicted efficiency: 0.0\n",
            "Actual efficiency:    2.755875\n",
            "Predicted score:      None\n",
            "Actual score:         1.0\n",
            "Rationale: \n",
            "\n",
            "=== Test Sample 9 ===\n",
            "SMILES: [H][C@]1(OC[C@H]2CCCCN(CCCCCCOC(C)OCCCCCC)CCCCCCOC(C)OCCCCCC)[C@]2([H])OC[C@@H]1OCCCN(CCCCCCOC(C)OCCCCCC)CCCCCCOC(C)OCCCCCC\n",
            "Predicted efficiency: 0.0\n",
            "Actual efficiency:    1.74017974121596\n",
            "Predicted score:      None\n",
            "Actual score:         2.0\n",
            "Rationale: \n",
            "\n",
            "=== Test Sample 10 ===\n",
            "SMILES: O=C1C(C(OCCCCCC)=O)CN(CCCN2CCCC2)CC1C(OCCCCCC)=O\n",
            "Predicted efficiency: 0.0\n",
            "Actual efficiency:    3.134814\n",
            "Predicted score:      None\n",
            "Actual score:         1.0\n",
            "Rationale: \n"
          ]
        }
      ]
    }
  ]
}